<p align="left">
     中文&nbsp ｜ &nbsp<a href="./README_en.md">English</a>
</p>
<div align="center">
   <img src="./web/荀子logonew.png" width="400"/>
</div>

# 荀子系列大語言模型

本次荀子系列模型開源包含兩個部分：基座模型**XunziALLM**，作為本次模型開源的重點，本計畫推出了完全開放使用的古籍領域大模型，同時，為方便非人工智慧 領域人員更了解本次開源模型，我們使用一部分資料建立了對話模型**XunziChat**,使用者可使用與呼叫千問、Baichuan2、ChatGLM3等對應開源模型一致方法呼叫我們的古籍處理模型。

目前已發布的古籍大模型:

| 模型名稱 | 下載連結 |對應開源模型 |
| :----------------------------- | :----------------- ------------------------------------------ | :------ ----------|
|Xunzi-Qwen-7B（基座模型） | [連結](https://modelscope.cn/models/Xunzillm4cc/Xunzi-Qwen) |Qwen-7B基座模型（版本v1.0.5）|
|Xunzi-Qwen-7B-CHAT（對話模型） | [連結](https://modelscope.cn/models/Xunzillm4cc/Xunzi-Qwen-Chat) |Qwen-7B對話模型|
|Xunzi-GLM-6B（基座模型） | [連結](https://modelscope.cn/models/Xunzillm4cc/Xunzi-GLM) |ChatGLM3-6B基座模型|
|Xunzi-Baichuan-7B（基座模型） | [連結](https://modelscope.cn/models/Xunzillm4cc/Xunzi-Baichuan) |Baichuan2-7B基座模型|

Xunzi-Qwen-7B與Xunzi-Qwen-7B-CHAT調用方式與Qwen模型相同。

Xunzi-GLM-6B呼叫方式與ChatGLM3-6B模型相同。

Xunzi-Baichuan-7B呼叫方式與Baichuan2-7B模型相同。

## 新聞
- 2024/1/16 更新github專案開源模型清單。

## 荀子系列模型亮點：

* 古籍智慧標引，荀子模型具備強大的古籍文獻標引能力，能夠對古籍中的內容進行高品質主題標引，幫助研究人員快速了解文章主題。

![index](./examples/index.png)
* 古籍資訊抽取，荀子模型能夠自動從古籍中抽取關鍵訊息，如人物、事件、地點等，大大節省了研究人員的資訊整理時間。

![ner](./examples/ner.png)
* 詩歌生成：荀子模型也具備詩歌生成的能力，能夠根據給定的主題或關鍵字，自動產生符合文法規則和韻律要求的古詩，為詩詞愛好者提供創作靈感。

![poetry](./examples/poetry.png)
* 古籍高品質翻譯：對於那些難以理解的古籍文獻，荀子模型能夠提供高品質的翻譯服務，幫助研究人員更好地理解原文意義。

![translation](./examples/translation.png)
* 閱讀理解：荀子模型能夠對所給的古文文本進行分析解釋，實現古籍文本的自動閱讀。

![reading_comprehension](./examples/reading_comprehension.png)
* 詞法分析：荀子模型可以完成古籍文本的自動分詞和詞性標註，能夠有效提升語言學工作者的研究效率。

![pos](./examples/pos.png)
* 自動標點：荀子大模型可以快速完成古籍文本的斷句和標點，提升研究者以及業餘愛好者對古籍文本的閱讀體驗。

![punctuation](./examples/punctuation.png)

由於我們同時發布了基座模型，使用者也可以根據自己的需求，使用本地的訓練語料微調荀子基座模型，使得其能夠在古籍下游處理任務上取得更佳的處理性能。

## 宣告：

荀子系列大語言模型在處理中華古籍文本資訊方面展現了不錯的性能，不僅能夠準確剖析古籍文本的複雜性，還可以進一步挖掘中國傳統文化的豐富內涵。 然而，我們也清楚地認識到，本模型仍有許多需要改進和最佳化的地方。 因此，我們非常歡迎並鼓勵使用者對我們的模型提出寶貴的意見和建議，並且在後續工作中，我們將推出具有更好性能的新版本大語言模型。

大語言模型龐大的參數量也帶來了更多的隨機性，雖然我們在訓練資料選取時已經盡可能保證了資料的合規性，但由於資料和模型的複雜性，仍有可能存在一些無法避免的問題。 因此，如果由於使用本開源模型而導致的各種問題，包括但不限於資料安全問題、公共輿論風險，或模型被誤導、濫用、傳播或不當利用所帶來的任何風險和問題，我們將不 承擔任何責任。

此外，根據國家網信辦等七部門聯合發布的[《生成式人工智慧服務管理暫行辦法》](http://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm)，在訓練、使用本模型以及其他生成式模型，請依據相關法律法規，為建立和諧、健康、可持續的生成式人工智慧社群共同努力。

如果您在模型使用過程中有任何疑問，歡迎聯絡我們(zhaozhixiao@stu.njau.edu.cn)
